Training acoustic models with sequentially incoming data -- while both leveraging new data and avoiding the forgetting effect-- is an essential obstacle to achieving human intelligence level in speech recognition. An obvious approach to leverage data from a new domain (e.g., new accented speech) is to first generate a comprehensive dataset of all domains, by combining all available data, and then use this dataset to retrain the acoustic models. However, as the amount of training data grows, storing and retraining on such a large-scale dataset becomes practically impossible. To deal with this problem, in this study, we study several domain expansion techniques which exploit only the data of the new domain to build a stronger model for all domains. These techniques are aimed at learning the new domain with a minimal forgetting effect (i.e., they maintain original model performance). These techniques modify the adaptation procedure by imposing new constraints including (1) weight constraint adaptation (wca): keeping the model parameters close to the original model parameters; (2) elastic weight consolidation (ewc): slowing down training for parameters that are important for previously established domains; (3) soft kl-divergence (skld): restricting the kl-divergence between the original and the adapted model output distributions; and (4) hybrid skld-ewc: incorporating both skld and ewc constraints. We evaluate these techniques in an accent adaptation task in which we adapt a deep neural network (dnn) acoustic model trained with native english to three different english accents: australian, hispanic, and indian. The experimental results show that skld significantly outperforms ewc, and ewc works better than wca. The hybrid skld-ewc technique results in the best overall performance.