Recently, (blanchet, kang, and murhy 2016, and blanchet, and kang 2017) showed that several machine learning algorithms, such as square-root lasso, support vector machines, and regularized logistic regression, among many others, can be represented exactly as distributionally robust optimization (dro) problems. The distributional uncertainty is defined as a neighborhood centered at the empirical distribution. We propose a methodology which learns such neighborhood in a natural data-driven way. We show rigorously that our framework encompasses adaptive regularization as a particular case. Moreover, we demonstrate empirically that our proposed methodology is able to improve upon a wide range of popular machine learning estimators.