Rotation is among the long prevailing, yet still unresolved, hard challenges encountered in visual object tracking. The existing deep learning-based tracking algorithms use regular cnns that are inherently translation equivariant, but not designed to tackle rotations. In this paper, we first demonstrate that in the presence of rotation instances in videos, the performance of existing trackers is severely affected. To circumvent the adverse effect of rotations, we present rotation-equivariant siamese networks (re-siamnets), built through the use of group-equivariant convolutional layers comprising steerable filters. Siamnets allow estimating the change in orientation of the object in an unsupervised manner, thereby facilitating its use in relative 2d pose estimation as well. We further show that this change in orientation can be used to impose an additional motion constraint in siamese tracking through imposing restriction on the change in orientation between two consecutive frames. For benchmarking, we present rotation tracking benchmark (rtb), a dataset comprising a set of videos with rotation instances. Through experiments on two popular siamese architectures, we show that re-siamnets handle the problem of rotation very well and out-perform their regular counterparts. Further, re-siamnets can accurately estimate the relative change in pose of the target in an unsupervised fashion, namely the in-plane rotation the target has sustained with respect to the reference frame.